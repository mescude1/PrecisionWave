\documentclass{article}
\usepackage{minted}
\usepackage{textcomp}
\usepackage{graphicx} % For shifting tables
\usepackage{algpseudocode}

\title{EAFIT University - Department of Information and Computer Sciences}
\author{Mauricio Escudero Restrepo \\
        César Esteban Peñuela Cubides \\
        Diego Mesa Ospina}
\date{August 2024}

\begin{document}

\maketitle

\section{Objective}
Review the progress of the final project and the work methodologies of the workgroups

\section{Course}
Numerical Analisis

\section{Responsible Faculty Member}
Edwar Samir Posada Murillo

\section{Current Report Delivery Date}
October 1st 2024

\section[]{Numerical Methods}
    The following numerical methods will be presented in this document in the following manner, first a pseudo-code
    version of the method algorithm will be presented, after the code for the implementation of the method in both
    languages selected will be presented, finally proof for the execution and results of the methods will be provided.

    \subsection{Incremental Search}

    Incremental Search Method is a numerical method used to find the roots of a function
    (i.e., where the function equals zero).
    This method works by iteratively narrowing down an interval where a root
    lies.
    The goal is to find an approximation of the root with increasing precision.

        \subsubsection{Pseudo-code}
FUNCIÓN BusquedaIncremental(f, x0, h, Nmax):

    ENTRADA:
    - f: función continua para la cual se busca un cambio de signo.
    - x0: punto inicial.
    - h: tamaño del paso (incremento en cada iteración).
    - Nmax: número máximo de iteraciones.

    SALIDA:
    - a: extremo izquierdo del intervalo donde ocurre un cambio de signo.
    - b: extremo derecho del intervalo donde ocurre un cambio de signo.
    - iter: número de iteraciones realizadas.
    - data_frame: tabla de resultados con detalles de cada iteración.

    INICIO:
    1. Inicializar:
       - xant (x anterior) = x0.
       - fant = f(xant).
       - xact (x actual) = xant + h.
       - fact = f(xact).
       - Crear una lista vacía para almacenar los resultados.

    2. Bucle FOR desde i = 1 hasta Nmax:

       a. Si fant * fact < 0, esto indica un cambio de signo en el intervalo [xant, xact]:
          - Guardar el resultado de la iteración (i, xact, fact, error).
          - Romper el bucle (salir).

       b. Guardar los resultados actuales en la lista:
          - Almacenar los valores de la iteración: índice i, x actual, f(x actual), y error (diferencia entre x actual y x anterior).

       c. Actualizar los valores para la siguiente iteración:
          - xant = xact.
          - fant = fact.
          - xact = xant + h.
          - fact = f(xact).

    3. Crear un DataFrame con los resultados almacenados.

    4. Retornar:
       - xant como extremo izquierdo del intervalo.
       - xact como extremo derecho del intervalo.
       - i como número de iteraciones realizadas.
       - el DataFrame con los resultados.

FIN FUNCIÓN

        \subsubsection{Method Implementation}

            \paragraph{Python}

            \begin{minted}{python}
    def incremental_search(f, x0, h, Nmax):
        """
        This program finds an interval where f(x) has a sign change
        using the incremental search method.
        Inputs:
        f: continuous function
        x0: initial point
        h: step
        Nmax: maximum number of iterations

        Outputs:
        a: left endpoint of the interval
        b: right endpoint of the interval
        iter: number of iterations
        """

        # Initialization

        xant = x0
        fant = f(xant)
        xact = xant + h
        fact = f(xact)
        result_array = []
        # Loop
        for i in range(1, Nmax+1):
            if fant * fact < 0:
                result = {
                    'i': i,
                    'x_i': xact,
                    'f_xi': fact,
                    'e': abs(xact - xant)
                }
                result_array.append(result)
                break
            result = {
                'i': i,
                'x_i': xact,
                'f_xi': fact,
                'e': abs(xact - xant)
            }
            result_array.append(result)
            xant = xact
            fant = fact
            xact = xant + h
            fact = f(xact)

        # Result delivery

        a = xant
        b = xact
        iter = i
        result_data_frame = pd.DataFrame(result_array)
        return a, b, iter, result_data_frame
                    \end{minted}

    \paragraph{Rust}\label{paragraph:rust}

    \begin{minted}{Rust}
        use std::f64::EPSILON;

        // Define a struct to hold the result for each iteration
        #[derive(Debug)]
        struct SearchResult {
            iteration: usize,
            x_i: f64,
            f_xi: f64,
            error: f64,
        }

        // Function for incremental search in Rust
        fn incremental_search<F>(f: F, x0: f64, h: f64, nmax: usize) -> (f64, f64, usize, Vec<SearchResult>)
        where
            F: Fn(f64) -> f64,
        {
            // Initialization
            let mut xant = x0;
            let mut fant = f(xant);
            let mut xact = xant + h;
            let mut fact = f(xact);

            // Create a Vec to store the results
            let mut result_array: Vec<SearchResult> = Vec::new();

            // Iteration variable
            let mut iterations = 0;

            // Loop
            for i in 1..=nmax {
                iterations = i;

                // Store the current iteration result
                let result = SearchResult {
                    iteration: i,
                    x_i: xact,
                    f_xi: fact,
                    error: (xact - xant).abs(),
                };
                result_array.push(result);

                // Check for sign change
                if fant * fact < 0.0 {
                    break;
                }

                // Update
                xant = xact;
                fant = fact;
                xact = xant + h;
                fact = f(xact);

                // Stopping condition if the step size is too small
                if (xact - xant).abs() < EPSILON {
                    break;
                }
            }

            // Return the result: interval endpoints, number of iterations, and the results array
            (xant, xact, iterations, result_array)
        }
    \end{minted}

    \subsubsection{Method Tests}\label{subsec:method-tests}

    \paragraph{The test parameters are as follows:}
        \begin{itemize}
            \item {\textflorin = math.log((math.sin(x)**2) + 1 ) - 1/2}
            \item {x0 = -3}
            \item {step = 0.5}
            \item {Tol = 1x10-7}
            \item {N = 100}
        \end{itemize}

    \paragraph{Result:}
        Interval: [-2.5, -2.0], Iterations: 2

        \begin{table}[ht]
        \begin{tabular}{llll}
        i & x\_i & f\_xi     & e   \\
        1 & -2.5 & -0.193863 & 0.5 \\
        2 & -2.0 & 0.102578  & 0.5
        \end{tabular}\label{tab:table}
        \end{table}

    \subsection{Bisection}\label{subsec:bisection}

    The bisection method is based on the Intermediate Value Theorem, which states that if a continuous function changes
    sign over an interval, there is at least one root in that interval. The method systematically reduces the interval
    in which the root lies by repeatedly bisecting it.

    \subsubsection{Pseudo-code}
        FUNCIÓN MétodoDeBisección(f, a, b, tolerancia=1e-7, max_iteraciones=100):

            ENTRADA:
            - f: función a evaluar
            - a, b: intervalo [a, b] en el cual se busca la raíz
            - tolerancia: criterio de parada (opcional)
            - max_iteraciones: número máximo de iteraciones (opcional)

            SALIDA:
            - c: valor aproximado de la raíz
            - iteraciones: número de iteraciones realizadas
            - convergió: indicador de si el algoritmo convergió o no
            - df_result: tabla de resultados de las iteraciones

            SI f(a) * f(b) >= 0 ENTONCES:
                Lanzar un error ("f(a) y f(b) deben tener signos opuestos")

            c = (a + b) / 2  // Inicializar el punto medio
            lista_resultados = []  // Inicializar lista para guardar resultados de cada iteración

            PARA i DESDE 0 HASTA max_iteraciones HACER:
                c = (a + b) / 2  // Calcular el punto medio de [a, b]

                SI (b - a) < tolerancia O abs(f(c)) < tolerancia ENTONCES:
                    Crear un DataFrame con lista_resultados  // Convertir la lista a tabla
                    IMPRIMIR el DataFrame  // Mostrar tabla con resultados de iteraciones
                    DEVOLVER (c, i + 1, True, DataFrame)  // Algoritmo ha convergido

                // Determinar en qué mitad del intervalo se encuentra la raíz
                SI f(a) * f(c) < 0 ENTONCES:
                    b = c  // La raíz está en el intervalo [a, c]
                SI NO:
                    a = c  // La raíz está en el intervalo [c, b]

                // Guardar los resultados de la iteración actual
                resultado = { 'iteración': i, 'x_i': c, 'f(x_i)': f(c), 'error': abs(b - a) }
                lista_resultados.AGREGAR(resultado)

            // Si se alcanzó el número máximo de iteraciones sin converger
            DEVOLVER (c, max_iteraciones, False, lista_resultados)

        FIN FUNCIÓN

    \subsubsection{Method Implementation}
    \paragraph{Python}

    \begin{minted}{Python}
    def bisection_method(f, a, b, tolerance=1e-7, max_iterations=100):
        """
        Bisection method to find the root of f(x) in the interval [a, b].

        Parameters:
        f : function
            The function for which we are trying to find a root.
        a, b : float
            The interval [a, b] in which the root is located.
        tolerance : float, optional
            The stopping criterion for the algorithm.
        max_iterations : int, optional
            The maximum number of iterations to perform.

        Returns:
        c : float
            The approximate root.
        iterations : int
            The number of iterations performed.
        converged : bool
            Whether the algorithm converged or not.
        """
        if f(a) * f(b) >= 0:
            raise ValueError("f(a) and f(b) must have opposite signs.")
        c = (a + b) / 2  # Midpoint initial
        result_array = []
        for i in range(max_iterations):
            c = (a + b) / 2  # Midpoint
            if abs(b - a) < tolerance or abs(f(c)) < tolerance:
                df_result = pd.DataFrame(data=result_array)
                print(df_result)
                return c, i + 1, True, df_result  # Converged

            # Narrow the interval based on the sign of f(c)
            if f(a) * f(c) < 0:
                b = c  # The root is in [a, c]
            else:
                a = c  # The root is in [c, b]

            result = {'i': i, 'x_i': c, 'f_x_i': f(c), 'e': abs(b - a)}
            result_array.append(result)

        return c, max_iterations, False  # Did not converge within max_iterations

    \end{minted}

    \paragraph{Rust}
    \begin{minted}{Rust}
    #[derive(Debug)]
    struct BisectionResult {
        iteration: usize,
        x_i: f64,
        f_x_i: f64,
        error: f64,
    }

    // Bisection method function in Rust
    fn bisection_method<F>(
        f: F,
        mut a: f64,
        mut b: f64,
        tolerance: f64,
        max_iterations: usize,
    ) -> (f64, usize, bool, Vec<BisectionResult>)
    where
        F: Fn(f64) -> f64,
    {
        // Check if f(a) and f(b) have opposite signs
        if f(a) * f(b) >= 0.0 {
            panic!("f(a) and f(b) must have opposite signs.");
        }

        // Create a Vec to store the results for each iteration
        let mut result_array: Vec<BisectionResult> = Vec::new();

        let mut c = (a + b) / 2.0; // Midpoint initialization

        // Bisection loop
        for i in 0..max_iterations {
            c = (a + b) / 2.0; // Midpoint

            // Check if the solution has converged
            if (b - a).abs() < tolerance || f(c).abs() < tolerance {
                return (c, i + 1, true, result_array); // Converged
            }

            // Narrow the interval based on the sign of f(c)
            if f(a) * f(c) < 0.0 {
                b = c; // The root is in [a, c]
            } else {
                a = c; // The root is in [c, b]
            }

            // Store the result of the current iteration
            let result = BisectionResult {
                iteration: i + 1,
                x_i: c,
                f_x_i: f(c),
                error: (b - a).abs(),
            };
            result_array.push(result);
        }

        // If the loop finishes without converging, return the last midpoint and results
        (c, max_iterations, false, result_array) // Did not converge within max_iterations
    }
    \end{minted}

    \subsubsection{Method Test}\label{subsec:method-test}
        The test parameters are as follows:
        \begin{itemize}
            \item {\textflorin = math.log((math.sin(x)**2) + 1 ) - 1/2}
            \item {a = 0}
            \item {b = 1}
            \item {Tol = 1x10-7}
            \item {N = 100}
        \end{itemize}

        Result:
            - Root: 0.9364047050476074
            - Iterations: 21
            - Converged: True

        \begin{table}[ht]
        \begin{tabular}{llll}
        i  & x\_i               & f\_x\_i                 & e                   \\
        0  & 0.5                & -0.2931087267313766     & 0.5                 \\
        1  & 0.75               & -0.11839639385347844    & 0.25                \\
        2  & 0.875              & -0.036817690757380506   & 0.125               \\
        3  & 0.9375             & 0.0006339161592386899   & 0.0625              \\
        4  & 0.90625            & -0.017772289226861138   & 0.03125             \\
        5  & 0.921875           & -0.008486582211768012   & 0.015625            \\
        6  & 0.9296875          & -0.0039053586270640928  & 0.0078125           \\
        7  & 0.93359375         & -0.0016304381170096915  & 0.00390625          \\
        8  & 0.935546875        & -0.0004969353153196909  & 0.001953125         \\
        9  & 0.9365234375       & 6.882244496264622e-05   & 0.0009765625        \\
        10 & 0.93603515625      & -0.00021397350516405567 & 0.00048828125       \\
        11 & 0.936279296875     & -7.255478812057126e-05  & 0.000244140625      \\
        12 & 0.9364013671875    & -1.860984900181606e-06  & 0.0001220703125     \\
        13 & 0.93646240234375   & 3.348202684883006e-05   & 6.103515625e-05     \\
        14 & 0.936431884765625  & 1.581084516011355e-05   & 3.0517578125e-05    \\
        15 & 0.9364166259765625 & 6.975011174192858e-06   & 1.52587890625e-05   \\
        16 & 0.9364089965820312 & 2.5570333977986692e-06  & 7.62939453125e-06   \\
        17 & 0.9364051818847656 & 3.4802931392352576e-07  & 3.814697265625e-06  \\
        18 & 0.9364032745361328 & -7.564765268641693e-07  & 1.9073486328125e-06 \\
        19 & 0.9364042282104492 & -2.042232898902263e-07  & 9.5367431640625e-07
        \end{tabular}\label{tab:table2}
        \end{table}

\break

    \subsection{False Rule}\label{subsec:false_rule}

        The false position method (also known as the regula falsi method) is a root-finding technique used in numerical analysis.
        It is similar to the bisection method in that it iteratively narrows down an interval where a root of a function exists.
        However, instead of using the midpoint of the interval as in the bisection method, the false position method uses a
        more refined estimate by linearly interpolating the function between the endpoints.

        \subsubsection{Pseudo-code}
FUNCIÓN ReglaFalsa(f, a, b, tol, Nmax):

    ENTRADA:
    - f: función continua de la cual se busca la raíz.
    - a: punto extremo izquierdo del intervalo inicial.
    - b: punto extremo derecho del intervalo inicial.
    - tol: tolerancia que define el criterio de parada (cuando el error es suficientemente pequeño).
    - Nmax: número máximo de iteraciones permitidas.

    SALIDA:
    - x: solución aproximada.
    - iter: número de iteraciones realizadas.
    - err: error estimado.
    - result_array: lista de resultados con los valores de cada iteración (punto, valor de f(x) y error).

    INICIO:
    1. Calcular f(a) y f(b)  // fa = f(a), fb = f(b)

    2. Calcular el primer punto medio usando la regla falsa:
       pm = (fb * a - fa * b) / (fb - fa)  // pm es el primer punto de prueba

    3. Evaluar la función en el punto medio:
       fpm = f(pm)

    4. Inicializar un valor grande de error (E = 1000) para comenzar el ciclo.

    5. Inicializar un contador de iteraciones (cont = 1).

    6. Crear una lista vacía para almacenar los resultados de cada iteración (result_array).

    7. Mientras el error (E) sea mayor que la tolerancia (tol) Y el número de iteraciones sea menor que Nmax:
        a. Si f(a) * f(pm) < 0:
            - La raíz está en el intervalo [a, pm], entonces actualizar b = pm y fb = fpm.
        b. Si no:
            - La raíz está en el intervalo [pm, b], entonces actualizar a = pm y fa = fpm.

        c. Guardar el valor anterior de pm (p0 = pm).

        d. Calcular un nuevo punto medio usando la regla falsa:
           pm = (fb * a - fa * b) / (fb - fa)

        e. Evaluar la función en el nuevo punto medio:
           fpm = f(pm)

        f. Calcular el error como la diferencia entre el nuevo y el anterior punto medio:
           E = abs(pm - p0)

        g. Guardar los resultados de la iteración actual (número de iteración, valor del punto, valor de f(pm) y error) en result_array.

        h. Incrementar el contador de iteraciones (cont += 1).

    8. Cuando el ciclo termine (cuando se alcance la tolerancia o el número máximo de iteraciones):
        - La solución aproximada es x = pm.
        - El número de iteraciones es iter = cont.
        - El error estimado es err = E.

    9. Devolver la solución aproximada, el número de iteraciones, el error y el conjunto de resultados de las iteraciones.

FIN FUNCIÓN

        \subsubsection{Method Implementation}
        \paragraph{Python}
        \begin{minted}{Python}
    import math
    import pandas as pd


    def false_rule(f, a, b, tol, Nmax):
        """
        This program finds the solution to the equation f(x) = 0 en el interval [a, b] using the method of the false rule

        Inputs:
        f: continious function
        a: left endpoint of the initial interval
        b: right endpoint of the initial interval
        tol: tolerance
        Nmax: maximum number of iterations

        Outputs:
        x: aproximate solution
        iter: number of iterations
        err: estimated error
        """
        # Initialization
        fa = f(a)
        fb = f(b)
        pm = (fb * a - fa * b) / (fb - fa)
        fpm = f(pm)
        E = 1000  # Initial large error
        cont = 1  # Iteration counted

        # loop of the false rule method
        result_array = []
        while E > tol and cont < Nmax:
            if fa * fpm < 0:
                b = pm
                fb = fpm
            else:
                a = pm
                fa = fpm


            # Updating previous point
            p0 = pm
            pm = (fb * a - fa * b) / (fb - fa)
            fpm = f(pm)
            E = abs(pm - p0)

            result = {
                'i': cont,
                'x_i': pm,
                'f_xi': fpm,
                'e': abs(pm - p0)
            }
            result_array.append(result)

            cont += 1

        # Results
        x = pm
        iter = cont
        err = E
        return x, iter, err, result_array
        \end{minted}

    \paragraph{Rust}

    \begin{minted}{Rust}
        // Define a struct to store results for each iteration
        #[derive(Debug)]
        struct FalsePositionResult {
            iteration: usize,
            a: f64,
            b: f64,
            x_new: f64,
            f_x_new: f64,
            error: f64,
        }

        // False Position method in Rust
        fn false_position<F>(
        f: F,
        mut a: f64,
        mut b: f64,
        tol: f64,
        max_iter: usize,
        ) -> (f64, Vec<FalsePositionResult>)
        where
        F: Fn(f64) -> f64,
        {
        // Check if f(a) and f(b) have opposite signs
        if f(a) * f(b) >= 0.0 {
        panic!("The function must have opposite signs at the endpoints a and b.");
        }

        // Create a Vec to store the results for each iteration
        let mut result_array: Vec<FalsePositionResult> = Vec::new();

        // False position method loop
        for i in 0..max_iter {
        // Calculate the new point using the false position formula
        let x_new = b - (f(b) * (b - a)) / (f(b) - f(a));
        let f_x_new = f(x_new);

        // Store the result of the current iteration
        let result = FalsePositionResult {
            iteration: i + 1,
            a,
            b,
            x_new,
            f_x_new,
            error: f_x_new.abs(),
        };
        result_array.push(result);

        // Check if the result is within the tolerance
        if f_x_new.abs() < tol {
            return (x_new, result_array); // Converged
        }

        // Update the interval based on the sign of f(x_new)
        if f(a) * f_x_new < 0.0 {
            b = x_new;
        } else {
            a = x_new;
        }
        }

        // If no solution is found within the given number of iterations
        panic!("Maximum number of iterations reached without convergence.");
        }
     \end{minted}
    \subsubsection{Method Tests}

        Approximate solution:
            \begin{itemize}
                \item x = 0.9364045808798893
                \item Iterations = 5
                \item Error = 2.2097967899981086e-10
            \end{itemize}

        \begin{table}[ht]
        \begin{tabular}{llll}
        i & x\_i               & f\_xi                  & e                      \\
        1 & 0.9365060516656253 & 5.875600835791861e-05  & 0.0025656709474095596  \\
        2 & 0.9364047307426415 & 8.67825411532408e-08   & 0.00010132092298376083 \\
        3 & 0.936404581100869  & 1.2815393191090152e-10 & 1.4964177252885236e-07 \\
        4 & 0.9364045808798893 & 1.894040480010517e-13  & 2.2097967899981086e-10
        \end{tabular}\label{tab:table3}
        \end{table}

    \subsection{Fixed Point}\label{subsec:fixed_point}

    The fixed-point method is an iterative numerical technique used to solve equations of the form x=g(x)x = g(x)x=g(x).
    In this method, the goal is to find a value xxx such that g(x)=xg(x) = xg(x)=x, which is known as a fixed
    point of the function g(x)g(x)g(x).

    \subsubsection{Pseudo-code}
    FUNCIÓN MétodoPuntoFijo(g, x0, tol=1e-7, max_iter=1000):

    ENTRADA:
    - g: función para la iteración de punto fijo, se busca que x = g(x).
    - x0: aproximación inicial o valor inicial para la iteración.
    - tol: tolerancia para determinar si el método ha convergido (opcional, por defecto 1e-7).
    - max_iter: número máximo de iteraciones permitidas (opcional, por defecto 1000).

    SALIDA:
    - x: solución aproximada al punto fijo.
    - iteraciones: número de iteraciones realizadas.
    - convergió: un booleano que indica si el método convergió o no.
    - result_array: lista que contiene los detalles de cada iteración (valores de x, g(x), y el error).

    INICIO:
    1. Crear una lista vacía para almacenar los resultados de cada iteración (result_array).

    2. Inicializar x con el valor inicial x0.

    3. PARA i DESDE 0 HASTA max_iter HACER:
        a. Calcular el nuevo valor x_nuevo = g(x).

        b. Calcular el error como error = abs(x_nuevo - x).

        c. Crear un diccionario con los resultados de la iteración actual:
           - i: número de iteración.
           - x: valor de x_nuevo.
           - g_x: valor de g(x_nuevo).
           - error: error calculado.
           Añadir este diccionario a result_array.

        d. SI el error es menor que la tolerancia (error < tol):
            - El método ha convergido.
            - Devolver x_nuevo, número de iteraciones (i + 1), True (convergió), y result_array.

        e. Actualizar el valor de x con x_nuevo.

    4. SI se alcanza el número máximo de iteraciones sin converger:
        - Devolver el último valor de x, max_iter, False (no convergió), y result_array.

FIN FUNCIÓN

    \subsubsection{Method Implementation}
    \paragraph{Python}
    \begin{minted}{Python}
    import pandas as pd


    def fixed_point_method(g, x0, tol=1e-7, max_iter=1000):
        """
        Fixed-Point Iteration Method to find a solution to x = g(x).

        Parameters:
        g : function
            The function to apply in the fixed-point iteration (x = g(x)).
        x0 : float
            Initial guess for the fixed-point.
        tol : float, optional
            Tolerance for convergence. Default is 1e-7.
        max_iter : int, optional
            Maximum number of iterations. Default is 1000.

        Returns:
        x : float
            The approximate fixed point.
        iterations : int
            The number of iterations performed.
        converged : bool
            Whether the method converged.
        result_array : list of dict
            A list containing the iteration details.
        """
        result_array = []
        x = x0

        for i in range(max_iter):
            x_new = g(x)
            error = abs(x_new - x)

            result = {
                'i': i + 1,
                'x': x_new,
                'g_x': g(x_new),
                'error': error
            }
            result_array.append(result)

            if error < tol:
                return x_new, i + 1, True, result_array  # Converged

            x = x_new  # Update for the next iteration

        return x, max_iter, False, pd.DataFrame(result_array)  # Did not converge within max_iter

    \end{minted}
    \paragraph{Rust}
    \begin{minted}{Rust}
        use std::f64;

        fn fixed_point_method<F>(g: F, x0: f64, tol: f64, max_iter: usize) -> (f64, usize, bool, Vec<(usize, f64, f64, f64)>)
        where
            F: Fn(f64) -> f64,
        {
            let mut result_array = Vec::new();
            let mut x = x0;

            for i in 0..max_iter {
                let x_new = g(x);
                let error = (x_new - x).abs();

                result_array.push((i + 1, x_new, g(x_new), error));

                if error < tol {
                    return (x_new, i + 1, true, result_array); // Converged
                }

                x = x_new; // Update for the next iteration
            }

            // Did not converge within max_iter
            (x, max_iter, false, result_array)
        }

        fn print_results(result_array: &Vec<(usize, f64, f64, f64)>) {
            println!("{:<10} {:<15} {:<15} {:<15}", "Iter", "x", "g(x)", "Error");

            for (i, x, g_x, error) in result_array {
                println!("{:<10} {:<15.8} {:<15.8} {:<15.8}", i, x, g_x, error);
            }
        }

        fn main() {
            // Example usage:
            let g = |x: f64| x.cos(); // Example function g(x) = cos(x)
            let x0 = 0.5; // Initial guess
            let tol = 1e-7; // Tolerance for convergence
            let max_iter = 1000; // Maximum number of iterations

            let (x, iterations, converged, result_array) = fixed_point_method(g, x0, tol, max_iter);

            println!("Fixed point: {}", x);
            println!("Iterations: {}", iterations);
            println!("Converged: {}", converged);

            // Print results in table-like format
            print_results(&result_array);
        }
    \end{minted}
    \subsubsection{Method Test}

    Approximate solution:
    \begin{itemize}
        \item {Root found: -0.37444505296105535}
        \item {Iterations: 30}
        \item {Error: 7.726074024994034e-08 }
    \end{itemize}
    \hspace*{-3cm} % Moves the table 2 cm into the left margin
    \makebox[\textwidth][l]{ % [l] aligns to the left
\begin{tabular}{|c|c|c|c|c|}
Iteration & x\_i                 & f(x\_i)                 & g(x\_i)              & Error                  \\
1         & -0.2931087267313766  & -0.12671281687488073    & -0.41982154360625734 & 0.2068912732686234     \\
2         & -0.41982154360625734 & 0.07351702442859243     & -0.3463045191776649  & 0.12671281687488073    \\
3         & -0.3463045191776649  & -0.044653937364644736   & -0.39095845654230965 & 0.07351702442859243    \\
4         & -0.39095845654230965 & 0.026553421648170428    & -0.3644050348941392  & 0.044653937364644736   \\
5         & -0.3644050348941392  & -0.016021268273817058   & -0.3804263031679563  & 0.026553421648170428   \\
6         & -0.3804263031679563  & 0.009589507887747428    & -0.37083679528020885 & 0.016021268273817058   \\
7         & -0.37083679528020885 & -0.005768850083372357   & -0.3766056453635812  & 0.009589507887747428   \\
8         & -0.3766056453635812  & 0.003460227756392209    & -0.373145417607189   & 0.005768850083372357   \\
9         & -0.373145417607189   & -0.002079223579867173   & -0.3752246411870562  & 0.003460227756392209   \\
10        & -0.3752246411870562  & 0.00124805513874654     & -0.37397658604830963 & 0.002079223579867173   \\
11        & -0.37397658604830963 & -0.0007496296601224861  & -0.3747262157084321  & 0.00124805513874654    \\
12        & -0.3747262157084321  & 0.00045008239797816874  & -0.37427613331045395 & 0.0007496296601224861  \\
13        & -0.37427613331045395 & -0.00027029514763832196 & -0.3745464284580923  & 0.00045008239797816874 \\
14        & -0.3745464284580923  & 0.0001623020232475736   & -0.3743841264348447  & 0.00027029514763832196 \\
15        & -0.3743841264348447  & -9.746439711039168e-05  & -0.3744815908319551  & 0.0001623020232475736  \\
16        & -0.3744815908319551  & 5.8525648058027624e-05  & -0.37442306518389706 & 9.746439711039168e-05  \\
17        & -0.37442306518389706 & -3.514467880877392e-05  & -0.37445820986270584 & 5.8525648058027624e-05 \\
18        & -0.37445820986270584 & 2.110401325022826e-05   & -0.3744371058494556  & 3.514467880877392e-05  \\
19        & -0.3744371058494556  & -1.2672877957420337e-05 & -0.37444977872741303 & 2.110401325022826e-05  \\
20        & -0.37444977872741303 & 7.609964212673681e-06   & -0.37444216876320036 & 1.2672877957420337e-05 \\
21        & -0.37444216876320036 & -4.5697420043566694e-06 & -0.3744467385052047  & 7.609964212673681e-06  \\
22        & -0.3744467385052047  & 2.744098679452467e-06   & -0.37444399440652526 & 4.5697420043566694e-06 \\
23        & -0.37444399440652526 & -1.647814738270359e-06  & -0.37444564222126353 & 2.744098679452467e-06  \\
24        & -0.37444564222126353 & 9.895019896788426e-07   & -0.37444465271927385 & 1.647814738270359e-06  \\
25        & -0.37444465271927385 & -5.941897863737111e-07  & -0.3744452469090602  & 9.895019896788426e-07  \\
26        & -0.3744452469090602  & 3.568071592630062e-07   & -0.37444489010190096 & 5.941897863737111e-07  \\
27        & -0.37444489010190096 & -2.1426045238026603e-07 & -0.37444510436235334 & 3.568071592630062e-07  \\
28        & -0.37444510436235334 & 1.28662038245686e-07    & -0.3744449757003151  & 2.1426045238026603e-07 \\
29        & -0.3744449757003151  & -7.726074024994034e-08  & -0.37444505296105535 & 1.28662038245686e-07   \\
30        & -0.37444505296105535 & 4.63945839523916e-08    & -0.3744450065664714  & 7.726074024994034e-08
\end{tabular}
}


    \subsection{Newton}
        \subsubsection{Pseudo-code}
        FUNCIÓN NewtonRaphson(f, df, x0, tol=1e-7, max_iter=30):

    ENTRADA:
    - f: función cuyo cero (raíz) se desea encontrar.
    - df: derivada de la función f.
    - x0: estimación inicial de la raíz.
    - tol: tolerancia para la convergencia (valor predeterminado es 1e-7).
    - max_iter: número máximo de iteraciones permitidas (valor predeterminado es 30).

    SALIDA:
    - x: raíz aproximada de la función f.
    - n: número de iteraciones realizadas.
    - result_array: tabla con los detalles de cada iteración.

    INICIO:
    1. Inicializar xn = x0 (estimación inicial).
    2. Crear una lista vacía result_array para almacenar los detalles de las iteraciones.

    3. Para n desde 0 hasta max_iter - 1 HACER:

        a. Calcular fxn = f(xn) y dfxn = df(xn) (la función y su derivada evaluadas en xn).

        b. Si |xn - (xn - fxn / dfxn)| < tol:
           - Guardar en result_array los detalles de la iteración actual (n, xi, f(xi), error).
           - Imprimir el DataFrame con los resultados de las iteraciones.
           - Retornar xn como la raíz aproximada.
           - Terminar la función.

        c. Si dfxn == 0 (la derivada es cero):
           - Imprimir "Derivada cero. No se encontró solución."
           - Retornar None (indica que no se encontró solución).

        d. Guardar en result_array los detalles de la iteración actual (n, xi, f(xi), error).

        e. Actualizar xn utilizando la fórmula del método de Newton-Raphson:
           - xn = xn - fxn / dfxn.

    4. Si se excede el número máximo de iteraciones:
       - Imprimir "Se excedió el número máximo de iteraciones. No se encontró solución."
       - Retornar None.

FIN FUNCIÓN

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def newton_raphson(f, df, x0, tol=1e-7, max_iter=30):
        """
        Solves f(x) = 0 using the Newton-Raphson method.

        Parameters:
        - f: The function whose root we want to find.
        - df: The derivative of the function f.
        - x0: Initial guess for the root.
        - tol: The tolerance for convergence (default is 1e-6).
        - max_iter: Maximum number of iterations (default is 100).

        Returns:
        - The estimated root and the number of iterations taken.
        """
        xn = x0
        result_array = []
        for n in range(0, max_iter):
            fxn = f(xn)
            dfxn = df(xn)

            if abs(xn - (xn - fxn / dfxn)) < tol:
                result = {
                    'i:': n,
                    'xi:': xn,
                    'f_xi:': fxn,
                    'E': abs(xn - (xn - fxn / dfxn))
                }
                result_array.append(result)
                print(f"Found solution after {n} iterations.")
                print(pd.DataFrame(result_array))
                return xn

            if dfxn == 0:
                print("Zero derivative. No solution found.")
                return None

            result = {
                'i:': n,
                'xi:': xn,
                'f_xi:': fxn,
                'E': abs(xn - (xn - fxn / dfxn))
            }
            result_array.append(result)
            # Update the next approximation using Newton-Raphson formula
            xn = xn - fxn / dfxn
        print("Exceeded maximum iterations. No solution found.")
        return None
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    use std::f64::EPSILON;

    fn newton_raphson<F, DF>(f: F, df: DF, x0: f64, tol: f64, max_iter: usize)
    -> Option<(f64, Vec<(usize, f64, f64, f64)>)>
    where
        F: Fn(f64) -> f64,
        DF: Fn(f64) -> f64,
    {
        let mut xn = x0;
        let mut result_array = Vec::new();

        for n in 0..max_iter {
            let fxn = f(xn);
            let dfxn = df(xn);

            // Avoid division by zero
            if dfxn.abs() < EPSILON {
                println!("Zero derivative encountered. No solution found.");
                return None;
            }

            // Calculate the error
            let error = (xn - (xn - fxn / dfxn)).abs();

            // Append iteration details to result_array
            result_array.push((n, xn, fxn, error));

            // Check if the solution has converged
            if error < tol {
                println!("Found solution after {} iterations.", n);
                return Some((xn, result_array));
            }

            // Update xn using Newton-Raphson formula
            xn = xn - fxn / dfxn;
        }

        println!("Exceeded maximum iterations. No solution found.");
        None
    }

    fn main() {
        // Example function f(x) = x^2 - 2 (finding square root of 2)
        let f = |x: f64| x * x - 2.0;
        let df = |x: f64| 2.0 * x;

        let x0 = 1.0;
        let tol = 1e-7;
        let max_iter = 30;

        match newton_raphson(f, df, x0, tol, max_iter) {
            Some((root, result_array)) => {
                println!("Root: {}", root);
                println!("Iterations:");
                println!("| Iter |   x   | f(x)  | Error  |");
                println!("--------------------------------");
                for (i, xi, f_xi, e) in result_array {
                    println!("| {:4} | {:5.4} | {:5.4} | {:7.4} |", i, xi, f_xi, e);
                }
            }
            None => println!("No solution found."),
        }
    }
                \end{minted}
        \subsubsection{Method Test}
            The root is: 0.9364045808526077
            Found solution after 4 iterations.
            \begin{table}[]
            \begin{tabular}{llll}
            i: & xi:                & f\_xi:                  & E                      \\
            0  & 1.2                & 0.12524132043913228     & 0.3464852988384717     \\
            1  & 0.8535147011615283 & -0.05025900826059804    & 0.07953835548275334    \\
            2  & 0.9330530566442816 & -0.0019446986420873502  & 0.003344827656164062   \\
            3  & 0.9363978843004457 & -3.877863362977685e-06  & 6.696552162011038e-06  \\
            4  & 0.9364045808526077 & -1.5608903058961232e-11 & 2.6954660725664326e-11
            \end{tabular}
            \end{table}
    \subsection{Secant}
        \subsubsection{Pseudo-code}
        Función MétodoSecante(f, x0, x1, tol, max_iter)
  Entrada:
    - f: función de la cual queremos encontrar una raíz.
    - x0: primer valor inicial.
    - x1: segundo valor inicial.
    - tol: tolerancia para determinar la convergencia (opcional, valor predeterminado 1e-7).
    - max_iter: número máximo de iteraciones permitidas (opcional, valor predeterminado 100).

  Salida:
    - La raíz aproximada de la función f o un mensaje de error si no converge.

  Inicializar una lista vacía llamada resultados.

  Para i desde 0 hasta max_iter - 1 hacer:
    1. Calcular el valor de f(x0) y f(x1).
    2. Si f(x1) es igual a f(x0), lanzar un error: "División por cero".
    3. Usar la fórmula del método de la secante para calcular x2:
       x2 = x1 - f(x1) * (x1 - x0) / (f(x1) - f(x0)).
    4. Si |x2 - x1| < tol, hacer lo siguiente:
        - Agregar a la lista resultados el índice i, el valor x1, el valor f(x2) y el error |x2 - x1|.
        - Imprimir la lista de resultados.
        - Imprimir: "Convergió después de i + 1 iteraciones."
        - Retornar x2 (la raíz).
    5. Si no ha convergido, agregar el resultado actual a la lista con el índice i, el valor x1, el valor f(x2) y el error |x2 - x1|.
    6. Actualizar x0 = x1 y x1 = x2 para la siguiente iteración.

  Si se alcanza el número máximo de iteraciones, lanzar un error: "El método de la secante no convergió dentro del número máximo de iteraciones."
Fin Función

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def secant_method(f, x0, x1, tol=1e-7, max_iter=100):
        """
        Secant method to find the root of a function f.

        Parameters:
        f : function
            The function for which we are trying to find a root.
        x0 : float
            Initial guess 1.
        x1 : float
            Initial guess 2.
        tol : float, optional
            Tolerance for stopping the iteration. Default is 1e-6.
        max_iter : int, optional
            Maximum number of iterations. Default is 100.

        Returns:
        float
            The root of the function f.
        """
        result_array = []

        for i in range(max_iter):
            # Calculate the value of f at the two initial guesses
            f_x0 = f(x0)
            f_x1 = f(x1)

            # Avoid division by zero
            if f_x1 == f_x0:
                raise ValueError("Division by zero encountered in the secant method.")

            # Secant method formula
            x2 = x1 - f_x1 * (x1 - x0) / (f_x1 - f_x0)

            # Check for convergence
            if abs(x2 - x1) < tol:
                result = {'i': i,
                          'x_i': x1,
                          'f_xi': x2,
                          'e': abs(x2 - x1)}
                result_array.append(result)
                print(DataFrame(result_array))
                print(f"Converged after {i + 1} iterations.")
                return x2

            # add results to list
            result = {'i': i,
                      'x_i': x1,
                      'f_xi': x2,
                      'e': abs(x2 - x1)}
            result_array.append(result)

            # Update guesses
            x0, x1 = x1, x2

        raise ValueError("Secant method did not converge within the maximum number of iterations.")
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    fn secant_method<F>(f: F, x0: f64, x1: f64, tol: f64, max_iter: usize) -> Result<(f64, Vec<(usize, f64, f64, f64)>), String>
        where
            F: Fn(f64) -> f64,
        {
            let mut x0 = x0;
            let mut x1 = x1;
            let mut result_array = Vec::new();

            for i in 0..max_iter {
                let f_x0 = f(x0);
                let f_x1 = f(x1);

                // Avoid division by zero
                if f_x1 == f_x0 {
                    return Err(String::from("Division by zero encountered in the secant method."));
                }

                // Secant method formula
                let x2 = x1 - f_x1 * (x1 - x0) / (f_x1 - f_x0);

                // Check for convergence
                let error = (x2 - x1).abs();
                result_array.push((i, x1, x2, error));

                if error < tol {
                    println!("Converged after {} iterations.", i + 1);
                    return Ok((x2, result_array));
                }

                // Update guesses for the next iteration
                x0 = x1;
                x1 = x2;
            }

            Err(String::from("Secant method did not converge within the maximum number of iterations."))
        }

        fn main() {
            // Example function f(x) = x^2 - 2 (finding square root of 2)
            let f = |x: f64| x * x - 2.0;

            let x0 = 1.0;
            let x1 = 2.0;
            let tol = 1e-7;
            let max_iter = 100;

            match secant_method(f, x0, x1, tol, max_iter) {
                Ok((root, result_array)) => {
                    println!("Root: {}", root);
                    println!("Iterations:");
                    println!("| Iter |   x_i   | f(x_i) |  Error  |");
                    println!("------------------------------------");
                    for (i, xi, f_xi, e) in result_array {
                        println!("| {:4} | {:7.4} | {:7.4} | {:7.4} |", i, xi, f_xi, e);
                    }
                }
                Err(err) => println!("{}", err),
            }
        }
                \end{minted}
        \subsubsection{Method Test}

    \subsection{Multiple Roots}
        \subsubsection{Pseudo-code}
        FUNCIÓN MetodoRaizMultiple(f, df, ddf, x0, tol=1e-7, max_iter=100):

    ENTRADA:
    - f: función de la cual se busca la raíz.
    - df: derivada de la función f.
    - ddf: segunda derivada de la función f.
    - x0: aproximación inicial de la raíz.
    - tol: tolerancia para la convergencia (opcional, valor por defecto 1e-7).
    - max_iter: número máximo de iteraciones permitidas (opcional, valor por defecto 100).

    SALIDA:
    - x: raíz aproximada de la función f.
    - iteraciones: número de iteraciones realizadas.
    - data_frame: tabla con los detalles de cada iteración (opcional).

    INICIO:

    1. Crear una lista vacía para almacenar los resultados de cada iteración (result_array).

    2. PARA i desde 0 hasta max_iter HACER:

        a. Evaluar f(x0), f'(x0) y f''(x0):
           - f_x0 = f(x0)
           - df_x0 = df(x0)
           - ddf_x0 = ddf(x0)

        b. Verificar si el denominador es cero:
           - Si [df_x0^2 - f_x0 * ddf_x0] es igual a 0, lanzar un error ("División por cero en el método de raíz múltiple").

        c. Calcular el nuevo valor de x utilizando la fórmula del método de raíz múltiple:
           - x1 = x0 - (f_x0 * df_x0) / (df_x0^2 - f_x0 * ddf_x0)

        d. Calcular el error como la diferencia absoluta:
           - error = abs(x1 - x0)

        e. Almacenar los resultados de la iteración actual en la lista:
           - Guardar (i, x0, f_x0, df_x0, ddf_x0, x1, error) en result_array.

        f. Verificar si el error es menor que la tolerancia (tol):
           - Si error < tol:
              - Crear un DataFrame con los resultados de result_array.
              - Imprimir el DataFrame.
              - Retornar x1, número de iteraciones (i + 1), y el DataFrame.

        g. Actualizar x0 para la siguiente iteración:
           - x0 = x1

    3. Si se alcanzó el número máximo de iteraciones sin converger, lanzar un error ("El método de raíz múltiple no convergió en el número máximo de iteraciones").

FIN FUNCIÓN

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def multiple_root_method(f, df, ddf, x0, tol=1e-7, max_iter=100):
        """
        Multiple Root Method to find the root of a function f where the root has multiplicity.

        Parameters:
        f : function
            The function whose root we want to find.
        df : function
            The derivative of the function f.
        ddf : function
            The second derivative of the function f.
        x0 : float
            Initial guess for the root.
        tol : float, optional
            Tolerance for stopping the iteration. Default is 1e-7.
        max_iter : int, optional
            Maximum number of iterations. Default is 100.

        Returns:
        float
            The root of the function f.
        int
            The number of iterations performed.
        pd.DataFrame
            DataFrame with iteration details.
        """
        result_array = []

        for i in range(max_iter):
            f_x0 = f(x0)
            df_x0 = df(x0)
            ddf_x0 = ddf(x0)

            if df_x0**2 - f_x0 * ddf_x0 == 0:
                raise ValueError("Division by zero encountered in the multiple root method.")

            # Multiple Root Method formula
            x1 = x0 - (f_x0 * df_x0) / (df_x0**2 - f_x0 * ddf_x0)

            # Check for convergence
            error = abs(x1 - x0)

            # Store iteration details in result_array
            result = {
                'i': i,
                'x_i': x0,
                'f(x_i)': f_x0,
                'df(x_i)': df_x0,
                'ddf(x_i)': ddf_x0,
                'x_(i+1)': x1,
                'Error': error
            }
            result_array.append(result)

            if error < tol:
                df_result = pd.DataFrame(result_array)
                print(df_result)
                print(f"Converged after {i + 1} iterations.")
                return x1, i + 1, df_result

            # Update x0 for next iteration
            x0 = x1

        raise ValueError("Multiple root method did not converge within the maximum number of iterations.")
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    use std::vec::Vec;

    #[derive(Debug)]
    struct IterationResult {
        i: usize,
        x_i: f64,
        f_xi: f64,
        df_xi: f64,
        ddf_xi: f64,
        x_next: f64,
        error: f64,
    }

    fn multiple_root_method(
        f: &dyn Fn(f64) -> f64,
        df: &dyn Fn(f64) -> f64,
        ddf: &dyn Fn(f64) -> f64,
        x0: f64,
        tol: f64,
        max_iter: usize,
    ) -> Result<(f64, usize, Vec<IterationResult>), &'static str> {
        let mut x0 = x0;
        let mut result_array: Vec<IterationResult> = Vec::new();

        for i in 0..max_iter {
            let f_x0 = f(x0);
            let df_x0 = df(x0);
            let ddf_x0 = ddf(x0);

            if df_x0.powi(2) - f_x0 * ddf_x0 == 0.0 {
                return Err("Division by zero encountered in the multiple root method.");
            }

            // Multiple Root Method formula
            let x1 = x0 - (f_x0 * df_x0) / (df_x0.powi(2) - f_x0 * ddf_x0);

            // Check for convergence
            let error = (x1 - x0).abs();

            // Store iteration details in result_array
            let result = IterationResult {
                i,
                x_i: x0,
                f_xi: f_x0,
                df_xi: df_x0,
                ddf_xi: ddf_x0,
                x_next: x1,
                error,
            };
            result_array.push(result);

            if error < tol {
                println!("Converged after {} iterations.", i + 1);
                return Ok((x1, i + 1, result_array));
            }

            // Update x0 for next iteration
            x0 = x1;
        }

        Err("Multiple root method did not converge within the maximum number of iterations.")
    }

    fn main() {
        // Example function: f(x) = x^3 - 3x^2 + 3x - 1
        // (has a root with multiplicity at x = 1)
        let f = |x: f64| x.powi(3) - 3.0 * x.powi(2) + 3.0 * x - 1.0;
        let df = |x: f64| 3.0 * x.powi(2) - 6.0 * x + 3.0; // First derivative of f(x)
        let ddf = |x: f64| 6.0 * x - 6.0;                 // Second derivative of f(x)

        let x0 = 0.5; // Initial guess
        let tol = 1e-7;
        let max_iter = 100;

        match multiple_root_method(&f, &df, &ddf, x0, tol, max_iter) {
            Ok((root, iterations, result_array)) => {
                println!("Root found: {}", root);
                println!("Number of iterations: {}", iterations);

                // Print result_array like a DataFrame
                println!("{: <4} {: <10} {: <10} {: <10} {: <10} {: <10} {: <10}",
                         "i", "x_i", "f(x_i)", "df(x_i)", "ddf(x_i)", "x_(i+1)", "Error");
                for result in result_array {
                    println!("{: <4} {: <10.6} {: <10.6} {: <10.6} {: <10.6} {: <10.6} {: <10.6}",
                             result.i, result.x_i, result.f_xi, result.df_xi, result.ddf_xi, result.x_next, result.error);
                }
            }
            Err(e) => {
                println!("{}", e);
            }
        }
    }
                \end{minted}
        \subsubsection{Method Test}

    \subsection{Gaussian Elimination No Pivot}
        \subsubsection{Pseudo-code}
        FUNCIÓN EliminacionGaussianaSinPivoteo(A, b):

    ENTRADA:
    - A: matriz de coeficientes (nxn).
    - b: vector del lado derecho (nx1).

    SALIDA:
    - x: vector solución al sistema de ecuaciones Ax = b.

    INICIO:
    1. Convertir A y b en matrices de tipo float, si es necesario.

    2. Crear la matriz aumentada uniendo A con b:
       - MatrizAumentada = [A | b]

    3. Para i desde 0 hasta n-1 HACER:  (n es la longitud de b)

        a. Verificar si el pivote diagonal MatrizAumentada[i, i] es 0. Si lo es, lanzar un error ("Se encontró un pivote cero en la fila i").

        b. **Eliminación hacia adelante**: Para cada fila j desde i+1 hasta n-1:
            - Calcular el factor de eliminación: factor = MatrizAumentada[j, i] / MatrizAumentada[i, i].
            - Restar factor * fila i de la fila j para eliminar los elementos por debajo de la diagonal en la columna i.

    4. **Sustitución hacia atrás**:

        a. Inicializar el vector solución x de longitud n con ceros.

        b. Para i desde n-1 hasta 0 HACER: (recorrer de abajo hacia arriba)
           - Calcular x[i] usando la fórmula:
             x[i] = (MatrizAumentada[i, -1] - suma de los productos de los elementos ya resueltos) / MatrizAumentada[i, i]

    5. Retornar el vector solución x.

FIN FUNCIÓN

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def gaussian_elimination_no_pivoting(A, b):
        """
        Solves the system of linear equations Ax = b using Gaussian Elimination without Pivoting.

        Parameters:
        A (list of lists or np.ndarray): Coefficient matrix.
        b (list or np.ndarray): Right-hand side vector.

        Returns:
        np.ndarray: Solution vector x.
        """
        # Convert A and b into numpy arrays
        A = np.array(A, float)
        b = np.array(b, float)
        n = len(b)

        # Augment A with b to form the augmented matrix
        augmented_matrix = np.hstack([A, b.reshape(-1, 1)])

        # Forward elimination (without pivoting)
        for i in range(n):
            # Check if the diagonal element is zero, which would lead to division by zero
            if augmented_matrix[i, i] == 0:
                raise ValueError(f"Zero pivot encountered at row {i}. No pivoting applied.")

            # Eliminate entries below the pivot
            for j in range(i + 1, n):
                factor = augmented_matrix[j, i] / augmented_matrix[i, i]
                augmented_matrix[j, i:] -= factor * augmented_matrix[i, i:]

        # Back substitution to solve for x
        x = np.zeros(n)
        for i in range(n - 1, -1, -1):
            x[i] = (augmented_matrix[i, -1] - np.dot(augmented_matrix[i, i + 1:], x[i + 1:])) / augmented_matrix[i, i]

        return x
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    fn gaussian_elimination_no_pivoting(a: &mut Vec<Vec<f64>>, b: &mut Vec<f64>) -> Vec<f64> {
        let n = b.len();

        // Forward elimination (without pivoting)
        for i in 0..n {
            // Check if the diagonal element is zero, which would lead to division by zero
            if a[i][i] == 0.0 {
                panic!("Zero pivot encountered at row {}. No pivoting applied.", i);
            }

            // Eliminate entries below the pivot
            for j in i + 1..n {
                let factor = a[j][i] / a[i][i];
                for k in i..n {
                    a[j][k] -= factor * a[i][k];
                }
                b[j] -= factor * b[i];
            }
        }

        // Back substitution to solve for x
        let mut x = vec![0.0; n];
        for i in (0..n).rev() {
            x[i] = b[i];
            for j in i + 1..n {
                x[i] -= a[i][j] * x[j];
            }
            x[i] /= a[i][i];
        }

        x
    }

    fn main() {
        // Example usage:
        let mut a = vec![
            vec![2.0, 1.0, -1.0],
            vec![-3.0, -1.0, 2.0],
            vec![-2.0, 1.0, 2.0],
        ];
        let mut b = vec![8.0, -11.0, -3.0];

        let solution = gaussian_elimination_no_pivoting(&mut a, &mut b);
        println!("Solution: {:?}", solution);
    }

                \end{minted}
        \subsubsection{Method Test}

    \subsection{Gaussian Elimination Partial Pivot}
        \subsubsection{Pseudo-code}
        FUNCIÓN EliminacionGaussianaConPivoteoParcial(A, b):

    ENTRADA:
    - A: matriz de coeficientes (nxn).
    - b: vector del lado derecho (nx1).

    SALIDA:
    - x: vector solución al sistema de ecuaciones Ax = b.

    INICIO:
    1. Convertir A y b en matrices de tipo float, si es necesario.

    2. Crear la matriz aumentada uniendo A con b:
       - MatrizAumentada = [A | b]

    3. Para i desde 0 hasta n-1 HACER:  (n es la longitud de b)

        a. **Pivoteo Parcial**: Encontrar la fila con el valor absoluto más grande en la columna i desde la fila i hasta la última.
           - max_fila = índice de la fila con el valor máximo en la columna i.
           - Si el valor en MatrizAumentada[max_fila, i] es 0, lanzar un error ("La matriz es singular o casi singular").

        b. Si la fila max_fila no es igual a i:
           - Intercambiar la fila i con la fila max_fila.

        c. **Eliminación hacia adelante**: Para cada fila j desde i+1 hasta n-1:
            - Calcular el factor de eliminación: factor = MatrizAumentada[j, i] / MatrizAumentada[i, i].
            - Restar factor * fila i de la fila j para eliminar los elementos por debajo de la diagonal en la columna i.

    4. **Sustitución hacia atrás**:

        a. Inicializar el vector solución x de longitud n con ceros.

        b. Para i desde n-1 hasta 0 HACER: (recorrer de abajo hacia arriba)
           - Calcular x[i] usando la fórmula:
             x[i] = (MatrizAumentada[i, -1] - suma de los productos de los elementos ya resueltos) / MatrizAumentada[i, i]

    5. Retornar el vector solución x.

FIN FUNCIÓN

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def gaussian_elimination_with_partial_pivoting(A, b):
        """
        Solves the system of linear equations Ax = b using Gaussian Elimination with Partial Pivoting.

        Parameters:
        A (list of lists or np.ndarray): Coefficient matrix.
        b (list or np.ndarray): Right-hand side vector.

        Returns:
        np.ndarray: Solution vector x.
        """
        # Convert A and b to numpy arrays
        A = np.array(A, float)
        b = np.array(b, float)
        n = len(b)

        # Augment A with b to form the augmented matrix
        augmented_matrix = np.hstack([A, b.reshape(-1, 1)])

        # Perform Gaussian elimination with partial pivoting
        for i in range(n):
            # Partial Pivoting: Find the row with the largest value in the current column
            max_row = np.argmax(abs(augmented_matrix[i:, i])) + i
            if augmented_matrix[max_row, i] == 0:
                raise ValueError("Matrix is singular or nearly singular")

            # Swap the current row with the row having the largest pivot element
            if max_row != i:
                augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]

            # Eliminate values below the pivot
            for j in range(i + 1, n):
                factor = augmented_matrix[j, i] / augmented_matrix[i, i]
                augmented_matrix[j, i:] -= factor * augmented_matrix[i, i:]

        # Back substitution to solve for x
        x = np.zeros(n)
        for i in range(n - 1, -1, -1):
            x[i] = (augmented_matrix[i, -1] - np.dot(augmented_matrix[i, i + 1:n],
            x[i + 1:])) / augmented_matrix[i, i]

        return x
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    fn gaussian_elimination_with_partial_pivoting(a: &mut Vec<Vec<f64>>, b: &mut Vec<f64>) -> Vec<f64> {
        let n = b.len();

        // Perform Gaussian elimination with partial pivoting
        for i in 0..n {
            // Partial Pivoting: Find the row with the largest value in the current column
            let mut max_row = i;
            for k in i + 1..n {
                if a[k][i].abs() > a[max_row][i].abs() {
                    max_row = k;
                }
            }

            // Check if the matrix is singular or nearly singular
            if a[max_row][i].abs() == 0.0 {
                panic!("Matrix is singular or nearly singular");
            }

            // Swap the current row with the row having the largest pivot element
            if max_row != i {
                a.swap(i, max_row);
                b.swap(i, max_row);
            }

            // Eliminate values below the pivot
            for j in i + 1..n {
                let factor = a[j][i] / a[i][i];
                for k in i..n {
                    a[j][k] -= factor * a[i][k];
                }
                b[j] -= factor * b[i];
            }
        }

        // Back substitution to solve for x
        let mut x = vec![0.0; n];
        for i in (0..n).rev() {
            x[i] = b[i];
            for j in i + 1..n {
                x[i] -= a[i][j] * x[j];
            }
            x[i] /= a[i][i];
        }

        x
    }

    fn main() {
        // Example usage:
        let mut a = vec![
            vec![2.0, 1.0, -1.0],
            vec![-3.0, -1.0, 2.0],
            vec![-2.0, 1.0, 2.0],
        ];
        let mut b = vec![8.0, -11.0, -3.0];

        let solution = gaussian_elimination_with_partial_pivoting(&mut a, &mut b);
        println!("Solution: {:?}", solution);
    }
                \end{minted}
        \subsubsection{Method Test}

    \subsection{Gaussian Elimination}
        \subsubsection{Pseudo-code}
        FUNCIÓN EliminacionGaussiana(A, b):

    ENTRADA:
    - A: matriz de coeficientes (nxn).
    - b: vector del lado derecho (nx1).

    SALIDA:
    - x: vector solución al sistema de ecuaciones Ax = b.

    INICIO:
    1. Convertir A y b en matrices de tipo float, si es necesario.

    2. Crear la matriz aumentada uniendo A con b:
       - MatrizAumentada = [A | b]

    3. Para i desde 0 hasta n-1 HACER:  (n es la longitud de b)

        a. **Pivot Parcial**: Encontrar la fila con el valor absoluto mayor en la columna i desde la fila i en adelante.
           - max_fila = índice de la fila con el valor absoluto más grande en la columna i.
           - Si el elemento en max_fila, i es 0, lanzar un error (la matriz es singular o casi singular).
           - Intercambiar la fila i con la fila max_fila.

        b. **Eliminación hacia adelante**: Para cada fila j desde i+1 hasta n-1:
            - Calcular el factor de eliminación: factor = MatrizAumentada[j, i] / MatrizAumentada[i, i].
            - Restar factor * fila i de la fila j para eliminar los elementos por debajo de la diagonal en la columna i.

    4. **Sustitución hacia atrás**:

        a. Inicializar el vector solución x de longitud n con ceros.

        b. Para i desde n-1 hasta 0 HACER: (recorrer de abajo hacia arriba)
           - Calcular x[i] usando la fórmula:
             x[i] = (MatrizAumentada[i, -1] - suma de los productos de los elementos ya resueltos) / MatrizAumentada[i, i]

    5. Retornar el vector solución x.

FIN FUNCIÓN

        \subsubsection{Method Implementation}
            \paragraph{Python}
                \begin{minted}{Python}
    def gaussian_elimination(A, b):
        """
        Solves the system of linear equations Ax = b using Gaussian Elimination.

        Parameters:
        A (list of list or np.ndarray): Coefficient matrix.
        b (list or np.ndarray): Right-hand side vector.

        Returns:
        np.ndarray: Solution vector x.
        """
        # Convert A and b into augmented matrix
        A = np.array(A, float)
        b = np.array(b, float)
        n = len(b)

        # Augment A with b
        augmented_matrix = np.hstack([A, b.reshape(-1, 1)])

        # Forward Elimination: Transform to upper triangular form
        for i in range(n):
            # Partial Pivoting: Swap rows if needed
            max_row = np.argmax(abs(augmented_matrix[i:, i])) + i
            if augmented_matrix[max_row, i] == 0:
                raise ValueError("Matrix is singular or nearly singular")
            augmented_matrix[[i, max_row]] = augmented_matrix[[max_row, i]]

            # Eliminate the below rows
            for j in range(i + 1, n):
                factor = augmented_matrix[j, i] / augmented_matrix[i, i]
                augmented_matrix[j, i:] -= factor * augmented_matrix[i, i:]

        # Back Substitution: Solve the upper triangular system
        x = np.zeros(n)
        for i in range(n - 1, -1, -1):
            x[i] = (augmented_matrix[i, -1] - np.dot(augmented_matrix[i, i + 1:n], x[i + 1:])) / augmented_matrix[i, i]

        return x
                \end{minted}
            \paragraph{Rust}
                \begin{minted}{Rust}
    fn gaussian_elimination(a: &mut Vec<Vec<f64>>, b: &mut Vec<f64>) -> Vec<f64> {
        let n = b.len();

        // Forward Elimination: Transform to upper triangular form
        for i in 0..n {
            // Partial Pivoting: Swap rows if needed
            let mut max_row = i;
            for k in i + 1..n {
                if a[k][i].abs() > a[max_row][i].abs() {
                    max_row = k;
                }
            }

            if a[max_row][i].abs() == 0.0 {
                panic!("Matrix is singular or nearly singular");
            }

            // Swap rows in both A and b
            a.swap(i, max_row);
            b.swap(i, max_row);

            // Eliminate the below rows
            for j in i + 1..n {
                let factor = a[j][i] / a[i][i];
                for k in i..n {
                    a[j][k] -= factor * a[i][k];
                }
                b[j] -= factor * b[i];
            }
        }

        // Back Substitution: Solve the upper triangular system
        let mut x = vec![0.0; n];
        for i in (0..n).rev() {
            x[i] = b[i];
            for j in i + 1..n {
                x[i] -= a[i][j] * x[j];
            }
            x[i] /= a[i][i];
        }

        x
    }

    fn main() {
        // Example usage:
        let mut a = vec![
            vec![2.0, 1.0, -1.0],
            vec![-3.0, -1.0, 2.0],
            vec![-2.0, 1.0, 2.0],
        ];
        let mut b = vec![8.0, -11.0, -3.0];

        let solution = gaussian_elimination(&mut a, &mut b);
        println!("Solution: {:?}", solution);
    }
                \end{minted}
        \subsubsection{Method Test}

\end{document}
